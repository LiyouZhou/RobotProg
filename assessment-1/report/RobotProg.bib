@online{AgilexAi,
  title = {Agilex.Ai},
  url = {https://global.agilex.ai/education/17},
  urldate = {2024-01-17},
  file = {/Users/leozhou/Zotero/storage/P7DLPCTN/17.html}
}

@inproceedings{anandCrackpotAutonomousRoad2018,
  title = {Crack-Pot: {{Autonomous Road Crack}} and {{Pothole Detection}}},
  shorttitle = {Crack-Pot},
  booktitle = {2018 {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  author = {Anand, Sukhad and Gupta, Saksham and Darbari, Vaibhav and Kohli, Shivam},
  date = {2018-12},
  pages = {1--6},
  doi = {10.1109/DICTA.2018.8615819},
  url = {https://ieeexplore.ieee.org/abstract/document/8615819},
  urldate = {2024-01-16},
  abstract = {With the advent of self-driving cars and autonomous robots, it is imperative to detect road impairments like cracks and potholes and to perform necessary evading maneuvers to ensure fluid journey for on-board passengers or equipment. We propose a fully autonomous robust real-time road crack and pothole detection algorithm which can be deployed on any GPU based conventional processing boards with an associated camera. The approach is based on a deep neural net architecture which detects cracks and potholes using texture and spatial features. We also propose pre-processing methods which ensure real-time performance. The novelty of the approach lies in using texture-based features to differentiate between crack surfaces and sound roads. The approach performs well in large viewpoint changes, background noise, shadows, and occlusion. The efficacy of the system is shown on standard road crack datasets.},
  eventtitle = {2018 {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  file = {/Users/leozhou/Zotero/storage/BKL8IE23/Anand et al. - 2018 - Crack-pot Autonomous Road Crack and Pothole Detec.pdf;/Users/leozhou/Zotero/storage/Q24GZLIK/8615819.html}
}

@article{brunoRobotizedRaspberryBasedSystem2023,
  title = {A {{Robotized Raspberry-Based System}} for {{Pothole 3D Reconstruction}} and {{Mapping}}},
  author = {Bruno, Salvatore and Loprencipe, Giuseppe and Di Mascio, Paola and Cantisani, Giuseppe and Fiore, Nicola and Polidori, Carlo and D’Andrea, Antonio and Moretti, Laura},
  date = {2023-01},
  journaltitle = {Sensors},
  volume = {23},
  number = {13},
  pages = {5860},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s23135860},
  url = {https://www.mdpi.com/1424-8220/23/13/5860},
  urldate = {2024-01-16},
  abstract = {Repairing potholes is a task for municipalities to prevent serious road user injuries and vehicle damage. This study presents a low-cost, high-performance pothole monitoring system to maintain urban roads. The authors developed a methodology based on photogrammetry techniques to predict the pothole’s shape and volume. A collection of overlapping 2D images shot by a Raspberry Pi Camera Module 3 connected to a Raspberry Pi 4 Model B has been used to create a pothole 3D model. The Raspberry-based configuration has been mounted on an autonomous and remote-controlled robot (developed in the InfraROB European project) to reduce workers’ exposure to live traffic in survey activities and automate the process. The outputs of photogrammetry processing software have been validated through laboratory tests set as ground truth; the trial has been conducted on a tile made of asphalt mixture, reproducing a real pothole. Global Positioning System (GPS) and Geographical Information System (GIS) technologies allowed visualising potholes on a map with information about their centre, volume, backfill material, and an associated image. Ten on-site tests validated that the system works in an uncontrolled environment and not only in the laboratory. The results showed that the system is a valuable tool for monitoring road potholes taking into account construction workers’ and road users’ health and safety.},
  issue = {13},
  langid = {english},
  keywords = {autonomous robot,digital photogrammetry processing software,GIS technologies,low-cost sensors,pavement potholes,Raspberry Pi,road surface monitoring and maintenance,safety at work,urban road networks},
  file = {/Users/leozhou/Zotero/storage/RIYJ6NEW/Bruno et al. - 2023 - A Robotized Raspberry-Based System for Pothole 3D .pdf}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2024-01-16},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/leozhou/Zotero/storage/GAEVW82Z/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/leozhou/Zotero/storage/2LWIUEGZ/1512.html}
}

@online{HomeORBBEC3D,
  title = {Home - {{ORBBEC}} - {{3D Vision}} for a {{3D World}}},
  url = {https://www.orbbec.com/},
  urldate = {2024-01-17},
  file = {/Users/leozhou/Zotero/storage/8AF2LMU2/www.orbbec.com.html}
}

@inproceedings{kangPotholeDetectionSystem2017,
  title = {Pothole Detection System Using {{2D LiDAR}} and Camera},
  booktitle = {2017 {{Ninth International Conference}} on {{Ubiquitous}} and {{Future Networks}} ({{ICUFN}})},
  author = {Kang, Byeong-ho and Choi, Su-il},
  date = {2017-07},
  pages = {744--746},
  issn = {2165-8536},
  doi = {10.1109/ICUFN.2017.7993890},
  url = {https://ieeexplore.ieee.org/abstract/document/7993890},
  urldate = {2024-01-16},
  abstract = {Automatic Pothole detection is important task for determining proper strategies of asphalt-surfaced pavement maintenance. In this paper, we develop a pothole detection system and method using 2D LiDAR and Camera. To improve the pothole detection accuracy, the combination of heterogeneous sensor system is used. By using 2D LiDAR, the distance and angle information of road are obtained. The pothole detection algorithm includes noise reduction pre-processing, clustering, line segment extraction, and gradient of pothole data function. Next, image-based pothole detection method is used to improve the accuracy of pothole detection and to obtain pothole shape. Image-based algorithm includes noise filtering, brightness control, binarization, addictive noise filtering, edge extraction, and object extraction and pothole detection. To show the pothole detection performance, experiments of pothole detection system using 2D LiDAR and camera are performed.},
  eventtitle = {2017 {{Ninth International Conference}} on {{Ubiquitous}} and {{Future Networks}} ({{ICUFN}})},
  file = {/Users/leozhou/Zotero/storage/CRUVJQPG/Kang and Choi - 2017 - Pothole detection system using 2D LiDAR and camera.pdf;/Users/leozhou/Zotero/storage/KBXVNYAB/7993890.html}
}

@inproceedings{macenskiMarathonNavigationSystem2020,
  title = {The {{Marathon}} 2: {{A Navigation System}}},
  shorttitle = {The {{Marathon}} 2},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Macenski, Steve and Martín, Francisco and White, Ruffin and Clavero, Jonatan Ginés},
  date = {2020-10-24},
  eprint = {2003.00368},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2718--2725},
  doi = {10.1109/IROS45743.2020.9341207},
  url = {http://arxiv.org/abs/2003.00368},
  urldate = {2024-01-17},
  abstract = {Developments in mobile robot navigation have enabled robots to operate in warehouses, retail stores, and on sidewalks around pedestrians. Various navigation solutions have been proposed, though few as widely adopted as ROS Navigation. 10 years on, it is still one of the most popular navigation solutions. Yet, ROS Navigation has failed to keep up with modern trends. We propose the new navigation solution, Navigation2, which builds on the successful legacy of ROS Navigation. Navigation2 uses a behavior tree for navigator task orchestration and employs new methods designed for dynamic environments applicable to a wider variety of modern sensors. It is built on top of ROS2, a secure message passing framework suitable for safety critical applications and program lifecycle management. We present experiments in a campus setting utilizing Navigation2 to operate safely alongside students over a marathon as an extension of the experiment proposed in Eppstein et al. The Navigation2 system is freely available at https://github.com/ros-planning/navigation2 with a rich community and instructions.},
  keywords = {Computer Science - Robotics},
  file = {/Users/leozhou/Zotero/storage/XBPA736N/Macenski et al. - 2020 - The Marathon 2 A Navigation System.pdf}
}

@inproceedings{omanovicPotholeDetectionImage2013,
  title = {Pothole {{Detection}} with {{Image Processing}} and {{Spectral Clustering}}},
  author = {Omanovic, Samir and Buza, Emir and Huseinovic, Alvin},
  date = {2013-10-01},
  abstract = {Potholes detection is one of the important tasks for the proper planning of repairs and rehabilitation of the asphalt surface pavement. Pothole repair is necessary in those situations where potholes compromise safety and pavement rideability. For detection and estimation of potholes, many of the existing methods usually used sophisticated equipments and costly tasks. In this paper we present method for rough detection and estimation of pothole by using image processing and spectral clustering. It is unsupervised vision-based method, which does not require additional filtering and training phase. Spectral clustering is used for identification regions by using histogram-based data from gray-scaled image. Based on these results, we identify potholes and estimate its surface. Method is tested on the images with different pothole shapes and the results show that this method estimates potholes with reasonable accuracy.}
}

@online{RACPotholeIndex,
  title = {{{RAC Pothole Index}} – Statistics and Data for {{UK}} Roads | {{RAC Drive}}},
  url = {https://www.rac.co.uk/drive/advice/driving-advice/rac-pothole-index-statistics-data-and-projections/},
  urldate = {2024-01-16},
  abstract = {July 2023, the latest pothole data, statistics and growth projections. Find everything you need to know about potholes here.},
  langid = {english},
  file = {/Users/leozhou/Zotero/storage/DMLZY8JH/rac-pothole-index-statistics-data-and-projections.html}
}

@online{RealTimePothole,
  title = {Real Time Pothole Detection Using {{Android}} Smartphones with Accelerometers | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/5982206},
  urldate = {2024-01-16},
  file = {/Users/leozhou/Zotero/storage/S93BE6Q7/5982206.html}
}

@online{RectLabelOfflineImage,
  title = {{{RectLabel}} - {{An}} Offline Image Annotation Tool for Object Detection and Segmentation},
  url = {https://rectlabel.com},
  urldate = {2024-01-16},
  abstract = {An offline image annotation tool for object detection and segmentation},
  organization = {{RectLabel}},
  file = {/Users/leozhou/Zotero/storage/DWG7YN7P/rectlabel.com.html}
}

@online{redmonYOLOv3IncrementalImprovement2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  shorttitle = {{{YOLOv3}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2018-04-08},
  eprint = {1804.02767},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1804.02767},
  url = {http://arxiv.org/abs/1804.02767},
  urldate = {2024-01-16},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/leozhou/Zotero/storage/ZZSEDK25/Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf;/Users/leozhou/Zotero/storage/UN3K9BY8/1804.html}
}

@online{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2016-01-06},
  eprint = {1506.01497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.01497},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2024-01-16},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/leozhou/Zotero/storage/5G5ALH4S/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf;/Users/leozhou/Zotero/storage/RUZHJ4EQ/1506.html}
}

@online{YDLIDARX2_YDLIDARFocus,
  title = {{{YDLIDAR X2}}\_{{YDLIDAR}}|{{Focus}} on Lidar Sensor Solutions},
  url = {https://www.ydlidar.com/products/view/6.html},
  urldate = {2024-01-17},
  file = {/Users/leozhou/Zotero/storage/JKCM4Q2Q/6.html}
}
